
\section{Część teoretyczna}
\subsection{Drzewo decyzyjne}
Przedmiotem klasyfikacji drzewa decyzyjnego są obiekty w danym zbiorze $U$
charakteryzowane przez pewien zestaw $D$ atrybutów nominalnych. Każdy obiekt
z $U$ ma $|D|$ atrybutów, z których każdy ma wartość ze skończonego zbioru.
Każdy obiekt w $U$ jest pewnej klasy, przy czym zbiór wszystkich klas to $Y$.

Zadanie polega na zbudowaniu klasyfikatora, który na podstawie atrybutów będzie
odgadywał klasy obiektów.

\subsection{Algorytm ID3: pseudokod}
\IncMargin{1em}
\begin{algorithm}[H]
  \SetAlgoLined
  \DontPrintSemicolon
  \caption{Iterative Dichotomiser 3}\label{ID3}
  \KwData{
    $
    Y: \text{zbiór klas},\newline
    D: \text{zbiór atrybutów wejściowych},\newline
    U \neq 0: \text{zbiór par uczących}
    $
  }
  \KwResult{Drzewo decyzyjne}
  \Begin{
    \If{$y_i == y$ $\quad$ $\forall$ $\langle$ $x_i$, $y_i$ $\rangle$ $\in$ $\text{U}$}
    {\Return $\text{Liść zawierający klasę } y.$}
    \If{$|\text{D}| == 0$}
    {\Return $\text{Liść zawierający najczęstszą klasę w } U.$}
    \emph{$d = \arg \max_{d \in D} \text{InfGain}(d, U)$}\;
    \emph{$U_j$ $=$ $\{$ $\langle$ $x_i$, $y_i$ $\rangle$ $\in$ $U$ : $x_i [d]$
    $= d_j\}$,$\textbf{ gdzie}$ \linebreak $d_j$} $\text{- j-ta wartość atrybutu }$ \emph{$d$}\;
    \emph{\Return}$\text{ Drzewo z korzeniem } d \text{ oraz krawiędziami: }\linebreak d_1,d_2,\dots \text{ prowadzącymi do drzew: }\linebreak ID3(Y, D-\{d\}, U_1), ID3(Y, D-\{d\}, U_2)\dots$\;
  }
\end{algorithm}

\subsection{Miara zróżnicowania - entropia}
Kluczowym elementem algorytmu jest wybór atrybutu przypisanego do korzenia
drzewa. Najlepiej byłoby wtedy, gdyby na podstawie atrybutu dało się podzielić
zbiór $U$ na podzbiory, takie że w każdym z nich występują wyłącznie obiekty
innej klasy. Nie jest to zwykle możliwe, dlatego stosuje się kryterium
zmierzające do stworzenia sytuacji zbliżonej, tj. jak największego zróżnicowania
występowania poszczęgólnych klas w podzbiorach. Miarą tego zróżnicowania jest
entropia:
$$I(U) = -\sum_{i}f_i\ln{f_i},$$
gdzie $f_i$ - częstość $i$-tej klasy.

Entropia zbioru podzielonego na podzbiory jest to średnia ważona entropii podzbiorów, a mianowicie
$$ Inf(d,U) = \sum_j \frac{S_j||}{|S|}I(S_j),$$
gdzie $|S|$ to liczba elementów  zbioru $S$, zaś $S_j$, $j=1,2,\dots$ to zbiory
powstałe przez podział zbioru $S$ ze wzsględu na wartość atrybutu $D$.

Zdobycz informacyjna służąca do wyboru atrybutu $d$ ma następującą definicję:
$$ InfGain(d,U) = I(U) - Inf(d,U).$$
